---
title: "Casey_Tilton_case_2"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```



```{r}

# libraries for EDA
library(tidyverse)
library(tidytext)
library(wordcloud)

#library for classifier
library(tm)
library(text2vec)
library(caret)
library(glmnet)
```

# Exploratory data analysis

```{r}
tweets <- read_csv("student_tm_case_training_data.csv")%>%
  rename(text = rawText, 'doc_id' = docID) %>%
  mutate(label = as.factor(label))%>%
  mutate(label = if_else(label == 0, "Non-political", "Political"))
  
```


```{r}
tweets_processed <- tweets %>%
  count(label, sort = TRUE)

ggplot(tweets_processed, aes(label, n))+
  geom_col(fill = "light blue")+
  labs(title = "Number of labeled political and non-political tweets",
      y = 'count of tweets')+
  theme_light()

```

### string cleaning script

```{r}
# Options & Functions
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')

tryTolower <- function(x){
  # return NA when there is an error
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error = function(e) e)
  # if not an error
  if (!inherits(try_error, 'error'))
    y = tolower(x)
  return(y)
}

# Create custom stop words
customStopwords <- c(stopwords('english'), "https", "t.co", "rt", "i'm", "it's", " it's", "it's ")

cleanCorpus<-function(corpus, customStopwords){
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url)) 
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tryTolower))
  corpus <- tm_map(corpus, removeWords, customStopwords)
  return(corpus)
}

txtCorpus <- VCorpus(DataframeSource(tweets))

# Preprocess the corpus
txtCorpus <- cleanCorpus(txtCorpus,customStopwords)


```

### Unigram count using regular tidytext stopwords antijoin

```{r}
tweets_unigram <- tweets %>%
  unnest_tokens(word, text)%>%
  anti_join(get_stopwords())%>%
  filter(!word %in% customStopwords)%>%
  count(word, label, sort = TRUE)%>%
  group_by(label)%>%
  slice_max(n, n = 50, with_ties = TRUE) %>%
  ungroup()%>%
  mutate(label = as.factor(label),
         word = reorder_within(word, n, label))
  
ggplot(tweets_unigram, aes(word, n)) + 
  geom_col(fill = "light blue")+
  coord_flip()+
  labs(title = "Most popular words in tweets labeled political and non-political",
       x = "Word",
       y = "Number of times word occurs in the dataset")+
  facet_wrap(~label, scales = "free")+
  scale_x_reordered()+
  theme_light()


```

### unigram count using special tweet tokenizer

```{r}
remove_reg <- "&amp;|&lt;|&gt;"

tweets_unigram <- tweets %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets")%>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !grepl("RT|http|@|rt", word))%>%
  count(word, label, sort = TRUE)%>%
  group_by(label)%>%
  slice_max(n, n = 30, with_ties = TRUE) %>%
  ungroup()%>%
  mutate(label = as.factor(label),
         word = reorder_within(word, n, label))
  
ggplot(tweets_unigram, aes(word, n)) + 
  geom_col(fill = "light blue")+
  coord_flip()+
  labs(title = "Most popular words in tweets labeled political and non-political",
       x = "Word",
       y = "Number of times word occurs in the dataset")+
  facet_wrap(~label, scales = "free")+
  scale_x_reordered()+
  theme_light()

```


### Unigram word cloud non-political

```{r}
tweets_unigram <- tweets %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets")%>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !grepl("RT|http|@|rt", word),
         label == "Non-political")%>%
  count(word, label, sort = TRUE)
set.seed(2021)
wordcloud(words = tweets_unigram$word, freq = tweets_unigram$n, min.freq = 5, max.words = 100, random.order = FALSE, rot.per = .35, colors = brewer.pal(8, "Dark2"))
```

### Unigram word cloud Political
```{r}
tweets_unigram <- tweets %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets")%>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !grepl("RT|http|@|rt", word),
         label == "Political")%>%
  count(word, label, sort = TRUE)
set.seed(2021)
wordcloud(words = tweets_unigram$word, scale=c(2,.2), freq = tweets_unigram$n, min.freq = 5, max.words = 100, random.order = FALSE, rot.per = .25, colors = brewer.pal(6, "Dark2"))
```

### Bigrams

```{r}
tweets_bigram <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  filter(!grepl("RT|https|@|rt",word1),
         !grepl("RT|https|@|rt", word2)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, label, sort = TRUE)%>%
  group_by(label)%>%
  slice_max(n, n = 20, with_ties = TRUE) %>%
  ungroup()%>%
  mutate(label = as.factor(label),
         bigram = reorder_within(bigram, n, label))
  
ggplot(tweets_bigram, aes(bigram, n)) + 
  geom_col(fill = "light blue")+
  coord_flip()+
  labs(title = "Most popular word pairs (bigrams) in tweets",
       x = "Bigram",
       y = "Number of times bigram occurs in the dataset")+
  facet_wrap(~label, scales = "free")+
  scale_x_reordered()+
  theme_light()
```

### bigram word cloud non political

```{r}
tweets_bigram <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  filter(!grepl("RT|https|@|rt",word1),
         !grepl("RT|https|@|rt", word2),
         label == "Non-political") %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, label, sort = TRUE)

set.seed(2021)
wordcloud(words = tweets_bigram$bigram, freq = tweets_bigram$n, min.freq = 5, max.words = 75, random.order = FALSE, rot.per = .4, colors = brewer.pal(6, "Dark2"))
```

### bigram word cloud Political

```{r}
tweets_bigram <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  filter(!grepl("RT|https|@|rt",word1),
         !grepl("RT|https|@|rt", word2),
         label == "Political") %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, label, sort = TRUE)

set.seed(2021)
wordcloud(words = tweets_bigram$bigram, scale = c(2, .2), freq = tweets_bigram$n, min.freq = 1, max.words = 75, random.order = FALSE, rot.per = .2, colors = brewer.pal(6, "Dark2"))
```


### tf_idf


```{r}
tweets_tf_idf <- tweets %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text)%>%
  filter(#!word %in% stop_words$word,
         #!word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"),
         !grepl("RT|http|@|rt", word))%>%
  count(word, label, sort = TRUE)%>%
  bind_tf_idf(word, label, n)%>%
  group_by(label)%>%
  slice_max(tf_idf, n = 20, with_ties = FALSE) %>%
  ungroup()%>%
  mutate(word = fct_reorder(word, tf_idf))

ggplot(tweets_tf_idf, aes(tf_idf, word, fill = label)) + 
  geom_col(show.legend = FALSE)+
  facet_wrap(~ label, scales = "free")+
  labs(title = "words that are most important to political and non-political tweets",
       subtitle = "but not common across both sets(tf-idf)",y = "", x = "term frequency-inverse document frequency score")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

### sentiment analysis

```{r}
afinn_tweets_sentiment <- tweets %>%
  unnest_tokens(word, text)%>%
  inner_join(get_sentiments("afinn"))%>%
  group_by(label) %>%
  summarise(sum_sentiment = sum(value),
            total_rows = n(),
            avg_sentiment = sum(value)/n())
```




```{r}
bing_tweets_sentiment <- tweets %>%
  unnest_tokens(word, text)%>%
  inner_join(get_sentiments("bing"))%>%
  mutate(sentiment_score = if_else(sentiment == "positive", 1, -1))%>%
  group_by(label) %>%
  summarise(sum_sentiment = sum(sentiment_score),
            total_rows = n(),
            avg_sentiment = sum(sentiment_score)/n())%>%
  view()
```

trying to combine both afinn and bing scores so I can summarise the average scores and display them in one table

``` {r}
afinn_tweets_sentiment <- tweets %>%
  unnest_tokens(word, text)%>%
  inner_join(get_sentiments("afinn"))%>%
  mutate(method = "AFINN")
  
            
bing_tweets_sentiment <- tweets %>%
  unnest_tokens(word, text)%>%
  inner_join(get_sentiments("bing"))%>%
  mutate(value = if_else(sentiment == "positive", 1, -1),
         method = "Bing")

combo <- bind_rows(afinn_tweets_sentiment, bing_tweets_sentiment)%>%
  mutate(row = row_number()) %>%
  pivot_wider(names_from = method,
              values_from = value,
              values_fill = 0)%>%
  group_by(label)%>%
  summarise(total_rows = n(),
            AFINN_mean = sum(AFINN)/n(),
            bing_mean = sum(Bing)/n())
```

# Creating a classifier model

This model will determine the likelihood that tweets in the test set are political or non-political tweets

```{r}
set.seed(2021)
# Read
model_tweets <- read.csv("student_tm_case_training_data.csv")%>%
  rename(text = rawText, 'doc_id' = docID)%>%
  mutate(label = as.factor(label))


### SAMPLE : Partitioning
idx <- createDataPartition(model_tweets$label, p=.8, list=F)
train_tweets <- model_tweets[idx,]
test_tweets  <- model_tweets[-idx,]

### EXPLORE
head(train_tweets$text,2)

table(train_tweets$label)

### MODIFY
# 
diagnosisClean<-function(xVec){
  xVec <- removePunctuation(xVec)
  xVec <- stripWhitespace(xVec)
  xVec <- tolower(xVec)
  return(xVec)
}

train_tweets$text <- diagnosisClean(train_tweets$text)

# Initial iterator to make vocabulary
iterMaker <- itoken(train_tweets$text, 
                    preprocess_function = list(tolower), 
                    progressbar         = T)
textVocab <- create_vocabulary(iterMaker, stopwords=stopwords('SMART'))
head(textVocab)
tail(textVocab)
nrow(textVocab)

#prune vocab to make DTM smaller
prunedtextVocab <- prune_vocabulary(textVocab,
                                    term_count_min = 10,
                                    doc_proportion_max = 0.5,
                                    doc_proportion_min = 0.001)
nrow(prunedtextVocab)

# Using the pruned vocabulary to declare the DTM vectors 
vectorizer <- vocab_vectorizer(prunedtextVocab)

# Take the vocabulary lexicon and the pruned text function to make a DTM 
tweetsDTM <- create_dtm(iterMaker, vectorizer)
dim(tweetsDTM)

### MODEL(s)
#train text only model
textFit <- cv.glmnet(tweetsDTM,
                     y=as.factor(train_tweets$label),
                     alpha=0.5,
                     family='binomial',
                     type.measure='auc',
                     nfolds=5,
                     intercept=F)


# Examine
head(coefficients(textFit),10)

# Subset to impacting terms
bestTerms <- subset(as.matrix(coefficients(textFit)), 
                    as.matrix(coefficients(textFit)) !=0)
bestTerms <- data.frame(tokens = rownames(bestTerms),
                        coeff  = bestTerms)
rownames(bestTerms) <- NULL
head(bestTerms[order(bestTerms$s1, decreasing = T),])
nrow(bestTerms)
ncol(tweetsDTM)

# Make training predictions
trainingPreds <- predict(textFit, tweetsDTM, type = 'class')
confusionMatrix(as.factor(trainingPreds),
                as.factor(train_tweets$label))

### Apply to test tweets requires the construction of the test tweet DTM exactly as the training set
testIT   <- itoken(test_tweets$text, 
                   tokenizer = word_tokenizer)

# Use the same vectorizer but with new iterator
testDTM <-create_dtm(testIT,vectorizer)

testPreds <- predict(textFit, testDTM, type = 'class')
confusionMatrix(as.factor(testPreds),
                as.factor(test_tweets$label))

# End of model creation and testing

```

### Apply model to unlabeled tweets dataset

```{r}
new_tweets <- read_csv("student_tm_case_score_data.csv")%>%
  rename(text = rawText, 'doc_id' = docID)

### Apply to new tweets requires the construction of the new tweet DTM exactly as the training set
newtweetsIT   <- itoken(new_tweets$text, 
                   tokenizer = word_tokenizer)

# Use the same vectorizer but with new iterator
newtweetsDTM <-create_dtm(newtweetsIT,vectorizer)

newPreds <- predict(textFit, newtweetsDTM, type = 'class')

new_tweets_preds <- data.frame(doc_id = new_tweets$doc_id, 
                              text = new_tweets$text,
                              newPreds)%>%
  rename(prediction = lambda.1se)%>%
  write_csv("casey_tilton_TMCase2_scores.csv")

preds_count <- new_tweets_preds %>%
  mutate(prediction = as.factor(prediction))%>%
  mutate(prediction = if_else(prediction == 0, "Non-political", "Political"))%>%
  count(prediction, sort = TRUE)
  
ggplot(preds_count, aes(prediction, n))+
  geom_col(fill = "light blue")+
  labs(title = "Number of predicted political and non-political tweets",
      y = 'count of tweets')+
  theme_light()
  

```

